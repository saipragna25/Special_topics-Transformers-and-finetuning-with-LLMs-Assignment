{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBZ3u+L1WnyrR7tsB5ejdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Transformers_and_finetuning_with_LLMs_A_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx6sGZw779-h",
        "outputId": "e163dc93-c674-4d8a-b902-9ce225316c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  174313\n",
            "\n",
            "First 1000 characters:\n",
            " ï»¿The Project Gutenberg eBook of Aliceâs Adventures in Wonderland, by Lewis Carroll\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this eBook or online at\r\n",
            "www.gutenberg.org. If you are not located in the United States, you\r\n",
            "will have to check the laws of the country where you are located before\r\n",
            "using this eBook.\r\n",
            "\r\n",
            "Title: Aliceâs Adventures in Wonderland\r\n",
            "\r\n",
            "Author: Lewis Carroll\r\n",
            "\r\n",
            "Release Date: January, 1991 [eBook #11]\r\n",
            "[Most recently updated: October 12, 2020]\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "Character set encoding: UTF-8\r\n",
            "\r\n",
            "Produced by: Arthur DiBianca and David Widger\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK ALICEâS ADVENTURES IN WONDERLAND ***\r\n",
            "\r\n",
            "[Illustration]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Aliceâs Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Unique characters: \n",
            "\r !\"#$%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz¹»¿Ãâï\n",
            "\n",
            "Vocab size: 96\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Downloading the \"Alice's Adventures in Wonderland\" dataset\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Displaying basic details about the text\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "print(\"\\nFirst 1000 characters:\\n\", text[:1000])\n",
        "\n",
        "# Unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Mapping from characters to integers and vice versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# Encoding and Decoding functions\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Encoding the entire dataset\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Splitting the data into train and validation sets\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(\"\\nUnique characters:\", ''.join(chars))\n",
        "print(\"\\nVocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- MODEL DEFINITION ----------------------\n",
        "\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(GPTBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_size, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_size, embed_size)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.ln1(x + attn_out)\n",
        "        ff_out = self.feed_forward(x)\n",
        "        return self.ln2(x + ff_out)\n",
        "\n",
        "class NanoGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_blocks):\n",
        "        super(NanoGPT, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.blocks = nn.ModuleList([GPTBlock(embed_size, num_heads) for _ in range(num_blocks)])\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        mask = torch.triu(torch.ones(len(x), len(x)), diagonal=1).bool().to(x.device)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "Z4PzhX2v-coM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------- DATA LOADER ----------------------\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.data[idx:idx+self.block_size], self.data[idx+1:idx+self.block_size+1])\n",
        "\n",
        "BLOCK_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = TextDataset(train_data, BLOCK_SIZE)\n",
        "val_dataset = TextDataset(val_data, BLOCK_SIZE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "YV6WTjGO-nY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 256  # This is typically set to the same value as EMBED_SIZE for simplicity\n",
        "NUM_LAYERS = 2  # Number of LSTM layers; adjust if desired\n",
        "vocab_size = len(itos)\n"
      ],
      "metadata": {
        "id": "JnaF6zXFai9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted NanoGPT model with dropout and layer normalization\n",
        "class NanoGPTWithRegularization(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout_prob=0.5):\n",
        "        super(NanoGPTWithRegularization, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Add dropout and layer normalization\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)  # Apply dropout after embedding\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = self.layer_norm(lstm_out)  # Apply layer normalization\n",
        "        x = self.fc(lstm_out)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zP-wTUInWCOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- TRAINING LOOP ----------------------\n",
        "\n",
        "EMBED_SIZE = 256\n",
        "NUM_HEADS = 4\n",
        "NUM_BLOCKS = 2\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "\n",
        "model = NanoGPT(vocab_size, EMBED_SIZE, NUM_HEADS, NUM_BLOCKS)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss = validate(model, val_loader, criterion)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4zWYgw7-iVo",
        "outputId": "8aa444c1-9ddc-43b7-fda1-2a9fcb312327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train loss: 2.3025, Val loss: 2.9730\n",
            "Epoch 2/10 - Train loss: 2.2948, Val loss: 3.0051\n",
            "Epoch 3/10 - Train loss: 2.2933, Val loss: 3.0330\n",
            "Epoch 4/10 - Train loss: 2.2926, Val loss: 3.0569\n",
            "Epoch 5/10 - Train loss: 2.2918, Val loss: 3.0573\n",
            "Epoch 6/10 - Train loss: 2.2914, Val loss: 3.0404\n",
            "Epoch 7/10 - Train loss: 2.2911, Val loss: 3.0550\n",
            "Epoch 8/10 - Train loss: 2.2908, Val loss: 3.0433\n",
            "Epoch 9/10 - Train loss: 2.2904, Val loss: 3.0585\n",
            "Epoch 10/10 - Train loss: 2.2904, Val loss: 3.0590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the Fine-tuning Dataset Preparation:"
      ],
      "metadata": {
        "id": "PxjfwKz0O8J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_dataset = TextDataset(val_data, BLOCK_SIZE)\n",
        "fine_tune_loader = DataLoader(fine_tune_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "ddODzTZJOmy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust Training Parameters:"
      ],
      "metadata": {
        "id": "aQPNKu4JPJj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FINE_TUNE_EPOCHS = 5\n",
        "FINE_TUNE_LR = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=FINE_TUNE_LR)\n"
      ],
      "metadata": {
        "id": "vAZp3n33OmnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning Training Loop:"
      ],
      "metadata": {
        "id": "kbQ_tik9PVox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(FINE_TUNE_EPOCHS):\n",
        "    train_loss = train(model, fine_tune_loader, optimizer, criterion)\n",
        "    print(f\"Fine-tuning Epoch {epoch+1}/{FINE_TUNE_EPOCHS} - Train loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKyziZLiOmcM",
        "outputId": "93bc0a4c-3ffd-428b-97ed-cbd3caed60df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning Epoch 1/5 - Train loss: 2.6301\n",
            "Fine-tuning Epoch 2/5 - Train loss: 2.4223\n",
            "Fine-tuning Epoch 3/5 - Train loss: 2.3842\n",
            "Fine-tuning Epoch 4/5 - Train loss: 2.3692\n",
            "Fine-tuning Epoch 5/5 - Train loss: 2.3608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the adjusted model\n",
        "model_regularized = NanoGPTWithRegularization(vocab_size, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "optimizer = torch.optim.Adam(model_regularized.parameters(), lr=0.001)\n",
        "\n",
        "# Fine-tuning the regularized model for additional epochs\n",
        "additional_epochs = 10\n",
        "for epoch in range(additional_epochs):\n",
        "    train_loss = train(model_regularized, fine_tune_loader, optimizer, criterion)\n",
        "    print(f\"Fine-tuning (Regularized) Epoch {epoch+1}/{additional_epochs} - Train loss: {train_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlM-DPM_UeOb",
        "outputId": "8706b625-8c1d-4e5b-8460-fa5064d89f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning (Regularized) Epoch 1/10 - Train loss: 0.8341\n",
            "Fine-tuning (Regularized) Epoch 2/10 - Train loss: 0.1255\n",
            "Fine-tuning (Regularized) Epoch 3/10 - Train loss: 0.0911\n",
            "Fine-tuning (Regularized) Epoch 4/10 - Train loss: 0.0816\n",
            "Fine-tuning (Regularized) Epoch 5/10 - Train loss: 0.0768\n",
            "Fine-tuning (Regularized) Epoch 6/10 - Train loss: 0.0743\n",
            "Fine-tuning (Regularized) Epoch 7/10 - Train loss: 0.0722\n",
            "Fine-tuning (Regularized) Epoch 8/10 - Train loss: 0.0706\n",
            "Fine-tuning (Regularized) Epoch 9/10 - Train loss: 0.0692\n",
            "Fine-tuning (Regularized) Epoch 10/10 - Train loss: 0.0682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, gen_length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode seed text\n",
        "        input_ids = torch.tensor([stoi[c] for c in seed_text], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate text\n",
        "        for _ in range(gen_length):\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :]\n",
        "            probs = nn.functional.softmax(logits / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = ''.join([itos[int(idx)] for idx in input_ids[0]])\n",
        "    return generated_text\n",
        "\n",
        "# Generate text with the trained model\n",
        "seed = \"Alice\"\n",
        "generated_sequence = generate_text(model_regularized, seed_text=seed, gen_length=1000)\n",
        "print(generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AobxlKtvMFCC",
        "outputId": "ca071fbe-ee57-4d84-9350-18f165ffb4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alicensed works that can be\r\n",
            "freely distributed in machine-readable form accessible by the widest\r\n",
            "array of equipment including outdated equipment. Many small donations\r\n",
            "($1 to $5,000) are particularly important to maintaining tax exempt\r\n",
            "status with the IRS.\r\n",
            "\r\n",
            "The Foundation is committed to complying with the laws regulating\r\n",
            "charities and charitable donations in all 50 states of the United\r\n",
            "States. Compliance requirements are not uniform and it takes a\r\n",
            "considerable effort, much paperwork and many fees to meet and keep up\r\n",
            "with these requirements. We do nations to the Project Gutenberg\r\n",
            "  Literary Archive Foundation.\"\r\n",
            "\r\n",
            "* You provide a full refund of any money paid by a user who notifies\r\n",
            "  you in writing (or by e-mail) within 30 days of receipt that s/he\r\n",
            "  does not agree to the user, processing or hypertext form. However, if you provide access\r\n",
            "to or distribute copies of a Project Gutenberg-tm work in a format\r\n",
            "other than \"Plain Vanilla ASCII\" or other format used in the official\r\n",
            "ver\n"
          ]
        }
      ]
    }
  ]
}