a) Implement nanogpt from scratch using your own code in pytorch, tensorflow and jax (3 colabs please). Take help from code interpreter gpt4 plugin. The code should be modular in colab and should have same caliber of debugging as the original colab 

You need to demonstrate the code as well as write a medium article explaining sections of code in depth, You need to use this to train a model for a book - pick differnet from 

shakespear book that is used in nanogpt

All artifacts need to be provided including colab, medium article, input, output, short presentation explaining the code (10 minutes) step by step

Link for Nanogpt implementation in pytorch Colab:

https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Transformers_and_finetuning_with_LLMs_A_Pytorch.ipynb

Link for nanogpt implementation in tensorflow colab: 

https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Transformers_and_finetuning_with_LLMs_A_Tensorflow.ipynb

Link for nanogpt implementation in JAX colab:

https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Transformers_Finetunung_with_LLms_A_Jax.ipynb

Link for Medium Article:

https://medium.com/@saipragna.kancheti/nanogpt-a-small-scale-gpt-for-text-generation-in-pytorch-tensorflow-and-jax-641c4efefbd5

Link for Presentation Slides:

https://docs.google.com/presentation/d/1KM2ZfRo9GqtNkvuumskRcE2qHy8tGjI3BJqgJCPPpIQ/edit?usp=sharing




b) Implement "textbooks are all you need" case study with your own data

full colab and all artifacts

Link for Colab:

https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Finetuning_with_LLMS_asgn_b_text_books_are_all_you_need.ipynb

