{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPtjIrt85gllNqSiVAjFuWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Transformers_and_finetuning_with_LLMs_A_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "pa5aLvCOa1B2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOZBgYvgadkX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n",
        "from tensorflow import shape as tf_shape\n",
        "from tensorflow import exp as tf_exp\n",
        "from tensorflow import square as tf_square\n",
        "from tensorflow import reduce_sum, reduce_mean\n",
        "from tensorflow import GradientTape\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Layer, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, MaxPooling2D, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.metrics import Mean, Metric\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.backend import random_normal\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "# from tensorflow.keras import saving\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 25\n",
        "    vocab_size: int = 200  # GPT-2 vocab_size of 50257, padded up to the nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12 # number of squential transformers\n",
        "    n_head: int = 12  # number of attention heads\n",
        "    n_embd: int = 768  # embedding size of the input\n",
        "    dropout: float = 0.2 # dropout percentage\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    epsilon: float = 1e-5  # epsilon value of layer normalization\n",
        "\n",
        "# Function to download the dataset\n",
        "def text_extractor(url = \"https://www.gutenberg.org/files/11/11-0.txt\"):\n",
        "    # Request to fetch the tiny shakespeare dataset\n",
        "    response = requests.get(url)\n",
        "    # Checking if we got a valid response\n",
        "    if response.status_code == 200:\n",
        "        # Opening a file and writing the content of the response\n",
        "        with open('input.txt', 'w') as file:\n",
        "            file.write(response.text)\n",
        "    else:\n",
        "        print(f\"Failed to get file with status code: {response.status_code}\")\n",
        "    # Reading the downloaded file\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Function to encode the text into numbers\n",
        "def text_encoder(text):\n",
        "    # Listing and sorting the unique characters in the text\n",
        "    chars = sorted(list(set(text)))\n",
        "    # Getting the total number of unique characters\n",
        "    vocab_size = len(chars)\n",
        "    print(\"\".join(chars))\n",
        "    print(vocab_size)\n",
        "    # Creating mappings from characters to their corresponding numerical representations\n",
        "    stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "    # Creating mappings from numbers to their corresponding characters\n",
        "    itos = {i:ch for i, ch in enumerate(chars)}\n",
        "    # Function to encode a string into a list of numbers\n",
        "    encode = lambda s: [stoi[ch] for ch in s]\n",
        "    # Function to decode a list of numbers back into a string\n",
        "    decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "    print(encode(\"hii this is pragna\"))\n",
        "    print(\"decoded: \", decode(encode(\"hii this is pragna\")))\n",
        "    # Encoding the entire text into numbers\n",
        "    series = encode(text)\n",
        "    n = int(0.8*len(series))\n",
        "    return series\n",
        "\n",
        "# Function to create a windowed dataset\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    # Creating a tensorflow dataset from the encoded series\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    # Creating a windowed dataset with each window of size window_size + 1 and shifting the window by 1 after each step\n",
        "    dataset = dataset.window(size=window_size+1, shift = 1, drop_remainder=True)\n",
        "    # Flattening the dataset\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
        "    # Splitting each window into features (all elements except the last) and target (the last element)\n",
        "    dataset = dataset.map(lambda x: (x[:-1], x[1:]))\n",
        "    # Shuffling the dataset\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "    # Batching the dataset and prefetching 1 batch at a time to improve performance\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    return dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "aqskI0xQbZme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.num_heads = config.n_head\n",
        "        self.head_size = config.n_embd // config.n_head\n",
        "\n",
        "        # Projecting input into key, query, and value for all attention heads, but in batch\n",
        "        self.c_attn = layers.Dense(3 * config.n_embd, use_bias=config.bias)\n",
        "\n",
        "        # Regularization\n",
        "        self.attn_dropout = layers.Dropout(config.dropout)\n",
        "        self.resid_dropout = layers.Dropout(config.dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Linear transformation for queries, keys, and values, note that C = n_embd\n",
        "        qkv = self.c_attn(x)  # Input shape: (B, T, C), Output shape: (B, T, 3 * n_embd)\n",
        "\n",
        "        # Split the queries, keys, and values\n",
        "        q, k, v = tf.split(qkv, 3, axis=-1)  # Input shape: (B, T, 3 * n_embd), Output shapes: 3 * (B, T, n_embd)\n",
        "\n",
        "\n",
        "        # Reshape queries, keys, and values for multi-head attention with head_size = n_embd // num_heads\n",
        "        # BUG: possible issue with tensorflow, you can use tf.reshape(q, (B, T, self.num_heads, -1)), for tensorflow B is unknown: it will give an error\n",
        "        q = tf.reshape(q, (-1, T, self.num_heads, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
        "        k = tf.reshape(k, (-1, T, self.num_heads, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
        "        v = tf.reshape(v, (-1, T, self.num_heads, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
        "\n",
        "\n",
        "        # Perform attention operations\n",
        "\n",
        "        # Transpose queries, keys, and values for efficient matrix multiplication\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
        "        k = tf.transpose(k, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
        "        v = tf.transpose(v, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
        "\n",
        "        # Compute attention scores (\"affinities\")\n",
        "        wei = tf.matmul(q, k, transpose_b=True) * (self.head_size ** -0.5)  # Output shape: (B, num_heads, T, T)\n",
        "\n",
        "        mask = tf.linalg.band_part(tf.ones_like(wei), -1, 0)  # Lower triangular matrix of ones\n",
        "        wei = tf.where(mask == 1, wei, float(\"-inf\"))  # Set upper triangular part to -inf\n",
        "\n",
        "        wei = tf.nn.softmax(wei, axis=-1)  # Output shape: (B, num_heads, T, T)\n",
        "        wei = self.attn_dropout(wei)  # Regularization step 1\n",
        "\n",
        "        # Perform the weighted aggregation of the values\n",
        "        out = tf.matmul(wei, v)  # Output shape: (B, num_heads, T, head_size)\n",
        "\n",
        "        # Transpose and reshape the output to match the original shape\n",
        "        out = tf.transpose(out, perm=[0, 2, 1, 3])  # Output shape: (B, T, num_heads, head_size)\n",
        "        out = tf.reshape(out, (-1, T, C))  # Output shape: (B, T, C) - note that C = num_heads * head_size = n_embd\n",
        "        out = self.resid_dropout(out)  # Regularization step 2\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "c-sToPzJbrAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(layers.Layer):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "        n_embed = config.n_embd\n",
        "        self.c_fc = layers.Dense(4 * n_embed, use_bias=config.bias, activation=tf.keras.activations.gelu)\n",
        "        self.c_proj = layers.Dense(config.n_embd, use_bias=config.bias)\n",
        "        self.dropout = layers.Dropout(config.dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YyPlNsDFe63V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(layers.Layer):\n",
        "    def __init__(self, config):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        # Layer normalizing the input data as the number of features increases over time\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=config.epsilon, center=False, scale=True)\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=config.epsilon, center=False, scale=True)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def call(self, x):\n",
        "        # 1. Input data is layer normalized: Layer normalizing the input data as the number of features increases over time\n",
        "        x_normalized = self.ln_1(x)\n",
        "\n",
        "        # 2. Fed through the attention network: We get the attention scores or weighted values\n",
        "        attn_output = self.attn(x_normalized)\n",
        "\n",
        "        # 3. Added to the input: Reduces vanishing gradient issues\n",
        "        x = x + attn_output\n",
        "\n",
        "        # 4. Layer normalized the data again\n",
        "        x_normalized = self.ln_2(x)\n",
        "\n",
        "        # 5. Final pass through a multi-layer perceptron: We are learning the features\n",
        "        mlp_output = self.mlp(x_normalized)\n",
        "\n",
        "        # 6. Added to the input again\n",
        "        x = x + mlp_output\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "Wvh8YM4Ehia9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(config):\n",
        "    \"\"\"\n",
        "    Creates an decoder model based on the provided configuration.\n",
        "\n",
        "    Args:\n",
        "        config: An object specifying the configuration parameters.\n",
        "\n",
        "    Returns:\n",
        "        decoder: A Keras Model object representing the encoder model.\n",
        "    \"\"\"\n",
        "\n",
        "    # create a dict with all the layers we need\n",
        "    transformer_dict = {\n",
        "        # input layer\n",
        "        'input': tf.keras.Input(shape=(config.block_size,)),\n",
        "        # word token embeddings\n",
        "        'wte': tf.keras.layers.Embedding(config.vocab_size, config.n_embd, input_length=config.block_size),\n",
        "        # word position embeddings\n",
        "        'wpe': tf.keras.layers.Embedding(config.block_size, config.n_embd),\n",
        "        # dropout layer\n",
        "        'drop': tf.keras.layers.Dropout(config.dropout),\n",
        "        # Transformer blocks\n",
        "        'h': tf.keras.Sequential([Block(config) for _ in range(config.n_layer)]),\n",
        "        # layer normalization\n",
        "        'ln_f': tf.keras.layers.LayerNormalization(epsilon=config.epsilon, center=False, scale=True),\n",
        "        # layer used to project the output of the GPT model to the vocabulary size\n",
        "        'lm_head': tf.keras.layers.Dense(config.vocab_size, use_bias=False)\n",
        "    }\n",
        "\n",
        "    # input\n",
        "    idx = transformer_dict['input']\n",
        "    pos = tf.range(0, config.block_size, dtype=tf.int64)  # shape (t)\n",
        "\n",
        "    # Forward the GPT model itself\n",
        "    tok_emb = transformer_dict['wte'](idx)  # token embeddings of shape (b, t, n_embd)\n",
        "    pos_emb = transformer_dict['wpe'](pos)  # position embeddings of shape (t, n_embd)\n",
        "    x = transformer_dict['drop'](tok_emb + pos_emb)\n",
        "    for block in transformer_dict['h'].layers:\n",
        "        x = block(x)\n",
        "    x = transformer_dict['ln_f'](x)\n",
        "\n",
        "    # logit scores for each vocabulary word at each position in the input sequence.\n",
        "    logits = transformer_dict['lm_head'](x)  # shape (batch_size, sequence_length, vocab_size)\n",
        "\n",
        "    # Create encoder model\n",
        "    model = tf.keras.Model(inputs=idx, outputs=logits, name='encoder')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "EuMIiE9GCrHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    config = GPTConfig\n",
        "    text = text_extractor()\n",
        "    series = text_encoder(text)\n",
        "    n = int(0.8*len(series))\n",
        "    train_dataset = windowed_dataset(series[:n], config.block_size, batch_size=250, shuffle_buffer=10)\n",
        "    test_dataset = windowed_dataset(series[n:], config.block_size, batch_size=250, shuffle_buffer=10)\n",
        "\n",
        "\n",
        "\n",
        "    # Create the decoder model\n",
        "    decoder_model = decoder(config)\n",
        "\n",
        "    # Compile and train the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    epochs = 10\n",
        "\n",
        "    decoder_model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "    history = decoder_model.fit(train_dataset, epochs=epochs, validation_data=test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90vpqFaaCuiY",
        "outputId": "fb564a3b-479b-4144-ce02-279247fdb9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz¹»¿Ãâï\n",
            "95\n",
            "[64, 65, 65, 1, 76, 64, 65, 75, 1, 65, 75, 1, 72, 74, 57, 63, 70, 57]\n",
            "decoded:  hii this is pragna\n",
            "Epoch 1/10\n",
            "546/546 [==============================] - 246s 391ms/step - loss: 2.9370 - val_loss: 2.8315\n",
            "Epoch 2/10\n",
            "546/546 [==============================] - 210s 385ms/step - loss: 2.1888 - val_loss: 2.5607\n",
            "Epoch 3/10\n",
            "546/546 [==============================] - 210s 384ms/step - loss: 1.9727 - val_loss: 2.4326\n",
            "Epoch 4/10\n",
            "546/546 [==============================] - 210s 384ms/step - loss: 1.8453 - val_loss: 2.3686\n",
            "Epoch 5/10\n",
            "546/546 [==============================] - 213s 391ms/step - loss: 1.7680 - val_loss: 2.3288\n",
            "Epoch 6/10\n",
            "546/546 [==============================] - 213s 390ms/step - loss: 1.7274 - val_loss: 2.2917\n",
            "Epoch 7/10\n",
            "546/546 [==============================] - 213s 390ms/step - loss: 1.7152 - val_loss: 2.3015\n",
            "Epoch 8/10\n",
            "546/546 [==============================] - 210s 384ms/step - loss: 1.7326 - val_loss: 2.3131\n",
            "Epoch 9/10\n",
            "546/546 [==============================] - 213s 390ms/step - loss: 1.8429 - val_loss: 2.4051\n",
            "Epoch 10/10\n",
            "546/546 [==============================] - 210s 384ms/step - loss: 1.9298 - val_loss: 2.4437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the learning rate for fine-tuning\n",
        "fine_tune_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Early stopping based on validation loss\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Continue training the model for fine-tuning\n",
        "fine_tune_epochs = 5\n",
        "\n",
        "decoder_model.compile(optimizer=fine_tune_optimizer, loss=loss_fn)\n",
        "history_fine_tune = decoder_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=fine_tune_epochs,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkqM5-HmtEmT",
        "outputId": "42d39a49-f41e-4e3d-e605-ea8bfd4a7293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "546/546 [==============================] - 237s 387ms/step - loss: 1.9003 - val_loss: 2.3474\n",
            "Epoch 2/5\n",
            "546/546 [==============================] - 211s 386ms/step - loss: 1.8348 - val_loss: 2.3274\n",
            "Epoch 3/5\n",
            "546/546 [==============================] - 210s 384ms/step - loss: 1.7979 - val_loss: 2.3072\n",
            "Epoch 4/5\n",
            "546/546 [==============================] - 210s 385ms/step - loss: 1.7711 - val_loss: 2.2977\n",
            "Epoch 5/5\n",
            "546/546 [==============================] - 213s 390ms/step - loss: 1.7501 - val_loss: 2.2914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, initial_prompt, generation_length=100):\n",
        "    generated_text = initial_prompt\n",
        "    for _ in range(generation_length):\n",
        "        # Convert the current prompt into model inputs\n",
        "        input_ids = tokenizer.encode(generated_text)\n",
        "\n",
        "        # Pad or truncate the input sequence to match the model's expected input length\n",
        "        if len(input_ids) > config.block_size:\n",
        "            input_ids = input_ids[-config.block_size:]\n",
        "        else:\n",
        "            padding_length = config.block_size - len(input_ids)\n",
        "            input_ids = [0] * padding_length + input_ids  # Prepend zeros for padding\n",
        "\n",
        "        input_ids = tf.expand_dims(input_ids, 0)\n",
        "\n",
        "        # Get predictions for the next token\n",
        "        logits = model(input_ids)\n",
        "        predictions = logits[:, -1, :]\n",
        "\n",
        "        # Sample the output (using tf.random.categorical) to generate token ID\n",
        "        token_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
        "\n",
        "        # Convert token ID back to character\n",
        "        token = tokenizer.decode([token_id])\n",
        "\n",
        "        # Append the token to the generated text\n",
        "        generated_text += token\n",
        "\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "4W600K6mrqIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, text):\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    def encode(self, s):\n",
        "        return [self.stoi[ch] for ch in s]\n",
        "\n",
        "    def decode(self, l):\n",
        "        return \"\".join([self.itos[i] for i in l])\n"
      ],
      "metadata": {
        "id": "8ESSjTvPrtFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer(text)\n",
        "initial_prompt = \"Alice\"\n",
        "generated_sequence = generate_text(decoder_model, tokenizer, initial_prompt, 500)\n",
        "print(generated_sequence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBAJvzUlrtQt",
        "outputId": "6ac90e18-347b-48f9-c05f-0fd7b95733dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice trougmed flound it them,  wad liffultled and thris hres, had and them for lead to he hundse not of ame triout to thes: begarnising not emell sain even the himpery roset up to the Mock Turtle âârat you,â the saited: ââWhis turs mead be a of the vorm!â\n",
            "\n",
            "\n",
            "Not sightid said to Alice.\n",
            "\n",
            "âWo went with that it cal of nevoce. ander to that the\n",
            "ips.\n",
            "\n",
            "âPay uectilled stup,â said the Gryphon con cared, âand the Queen sead canât ifuldedow\n",
            "the Queen.\n",
            "\n",
            "âSo so\n",
            "ques glow ited not fion the \n"
          ]
        }
      ]
    }
  ]
}