{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOao248n5VU3EYNOkS1imq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saipragna25/Special_topics-Transformers-and-finetuning-with-LLMs-Assignment/blob/main/ST_Transformers_Finetunung_with_LLms_A_Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax jaxlib flax optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za3tl5tj3wH5",
        "outputId": "750306db-0295-4387-a8e0-c95ce9348479"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.16)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (0.4.16+cuda11.cudnn86)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.3.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax) (1.11.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.0.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.1)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.7)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.16.1)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.5.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.5.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import pickle\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "from flax import serialization\n",
        "\n",
        "import optax\n",
        "import requests"
      ],
      "metadata": {
        "id": "6sHNBofEbweV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config():\n",
        "    seed = 42\n",
        "    num_iterations = 20000\n",
        "    batch_size = 512\n",
        "    block_size = 64\n",
        "    learning_rate = 1e-4\n",
        "    embed_size = 256\n",
        "    num_heads = 8\n",
        "    head_size = 32\n",
        "    num_layers = 6\n",
        "    dropout = 0.2\n",
        "\n",
        "config = Config()\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "    # Request to fetch the Alice dataset\n",
        "response = requests.get(url)\n",
        "    # Checking if we got a valid response\n",
        "if response.status_code == 200:\n",
        "        # Opening a file and writing the content of the response\n",
        "  with open('input.txt', 'w') as file:\n",
        "        file.write(response.text)\n",
        "else:\n",
        "  print(f\"Failed to get file with status code: {response.status_code}\")\n",
        "    # Reading the downloaded file\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "data = jnp.array(encode(text))\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "eval_data = data[n:]\n",
        "\n",
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "@jax.jit\n",
        "def get_batch(random_key, data):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    ix = jax.random.randint(random_key, shape=(config.batch_size, 1), minval=0, maxval=len(data)-config.block_size)\n",
        "    x = dynamic_slice_vmap(data, ix, (config.block_size,))\n",
        "    y = dynamic_slice_vmap(data, ix+1, (config.block_size,))\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "NpBPTJuHa8Bq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    epsilon: float = 1e-6\n",
        "    reduction_axes = -1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies layer normalization on the input.\"\"\"\n",
        "        # compute statistics\n",
        "        mean2 = jnp.mean(jax.lax.square(x), self.reduction_axes, keepdims=True)\n",
        "        mean = jnp.mean(x, self.reduction_axes, keepdims=True)\n",
        "        var = jnp.maximum(0., mean2 - jax.lax.square(mean))\n",
        "\n",
        "        # compute normalized inputs\n",
        "        x_norm = (x - mean) * jax.lax.rsqrt(var + self.epsilon)\n",
        "        return x_norm * self.param(\"scale\", nn.initializers.ones, x.shape[-1]) + self.param(\"bias\", nn.initializers.zeros, x.shape[-1])\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool):\n",
        "        key = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        query = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "        value = nn.Dense(self.head_size, use_bias=False)(x)\n",
        "\n",
        "        tril = jnp.tril(jnp.ones((x.shape[-2], x.shape[-2])))\n",
        "        attention_weights = nn.softmax(jnp.where(tril == 0, -jnp.inf, query @ jnp.transpose(key, axes=(0, 2, 1))), axis=-1)\n",
        "        attention_weights = nn.Dropout(config.dropout)(attention_weights, deterministic=not training)\n",
        "        return attention_weights @ value"
      ],
      "metadata": {
        "id": "vFevioLFbqs8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool):\n",
        "        x = jnp.concatenate([Attention(self.head_size)(x, training) for _ in range(self.num_heads)], axis=-1)\n",
        "        return nn.Dropout(config.dropout)(nn.Dense(self.num_heads*self.head_size)(x), deterministic=not training)\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool):\n",
        "        return nn.Dropout(config.dropout)(nn.Dense(config.embed_size)(nn.relu(nn.Dense(4*config.embed_size)(x))), deterministic=not training)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool):\n",
        "        x = x + MultiHeadAttention(self.num_heads, self.head_size)(LayerNorm()(x), training)\n",
        "        return x + FeedFoward()(LayerNorm()(x), training)\n"
      ],
      "metadata": {
        "id": "4am_Y8tVbmur"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    head_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool):\n",
        "        B, T = x.shape\n",
        "        x = nn.Embed(num_embeddings=vocab_size, features=config.embed_size)(x) + \\\n",
        "            nn.Embed(num_embeddings=config.block_size, features=config.embed_size)(jnp.arange(T))\n",
        "        for _ in range(self.num_layers):\n",
        "            x = Block(self.num_heads, self.head_size)(x, training)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        return nn.Dense(vocab_size)(x)\n",
        "\n",
        "    def generate(self, random_key, params, context, length=50):\n",
        "        for _ in range(length):\n",
        "            logits = self.apply(params, context[:, -config.block_size:], training=False)\n",
        "            random_key, random_subkey = jax.random.split(random_key)\n",
        "            new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=(1, 1))\n",
        "            context = jnp.concatenate([context, new_token], axis=1)\n",
        "        return context\n",
        "\n",
        "    @partial(jax.jit, static_argnames=(\"self\", \"length\"))\n",
        "    def generate_jit(self, random_key, params, length):\n",
        "        def scan_generate(carry, x):\n",
        "            key, context = carry\n",
        "            logits = self.apply(params, context, training=False)\n",
        "            random_key, random_subkey = jax.random.split(key)\n",
        "            new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=(1, 1))\n",
        "            context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n",
        "            return (random_key, context), new_token\n",
        "\n",
        "        _, new_tokens = jax.lax.scan(\n",
        "            scan_generate,\n",
        "            (random_key, jnp.zeros((1, config.block_size), dtype=jnp.int32)),\n",
        "            (),\n",
        "            length=length,\n",
        "        )\n",
        "        return new_tokens"
      ],
      "metadata": {
        "id": "hYEWlcu8bgj2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "  key: jax.random.KeyArray\n",
        "\n",
        "def create_train_state(random_key, config):\n",
        "    model = Model(num_layers=config.num_layers, num_heads=config.num_heads, head_size=config.head_size)\n",
        "    params = model.init(random_key, jnp.ones((config.batch_size, config.block_size), dtype=jnp.int32), training=False)\n",
        "    tx = optax.adamw(config.learning_rate)\n",
        "    return TrainState.create(\n",
        "        apply_fn=model.apply, params=params, key=random_key, tx=tx)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y, dropout_key):\n",
        "    dropout_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn(params, x, training=True, rngs={'dropout': dropout_key})\n",
        "        one_hot_encoded_labels = jax.nn.one_hot(y, num_classes=vocab_size)\n",
        "        return optax.softmax_cross_entropy(\n",
        "            logits=logits, labels=one_hot_encoded_labels\n",
        "        ).mean()\n",
        "\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    return state, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCtJGXSibSCi",
        "outputId": "e12cc6b6-9275-44be-af94-76f27ffb911c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-89a35cc15ddf>:2: DeprecationWarning: jax.random.KeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n",
            "  key: jax.random.KeyArray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(state, x, y):\n",
        "    logits = state.apply_fn(state.params, x, training=False)\n",
        "    one_hot_encoded_labels = jax.nn.one_hot(y, num_classes=vocab_size)\n",
        "    return optax.softmax_cross_entropy(\n",
        "        logits=logits, labels=one_hot_encoded_labels\n",
        "    ).mean()\n",
        "\n",
        "random_key = jax.random.PRNGKey(config.seed)\n",
        "random_key, random_subkey = jax.random.split(random_key)\n",
        "\n",
        "state = create_train_state(random_subkey, config)\n",
        "for i in range(config.num_iterations):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "    state, loss = train_step(state, *get_batch(random_subkey, train_data), random_subkey)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        random_key, random_subkey = jax.random.split(random_key)\n",
        "        print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_step(state, *get_batch(random_subkey, eval_data))}\")\n",
        "\n",
        "params_state_dict = serialization.to_state_dict(state.params)\n",
        "with open(\"./outputs/params.pickle\", \"wb\") as params_file:\n",
        "    pickle.dump(params_state_dict, params_file)\n",
        "\n",
        "# Let's now generate some text\n",
        "model = Model(num_layers=config.num_layers, num_heads=config.num_heads, head_size=config.head_size)\n",
        "params = model.init(\n",
        "    random_key, jnp.ones((config.batch_size, config.block_size), dtype=jnp.int32), training=False\n",
        ")\n",
        "with open(\"./outputs/params.pickle\", \"rb\") as params_file:\n",
        "    params_state_dict = pickle.load(params_file)\n",
        "params = serialization.from_state_dict(params, params_state_dict)\n",
        "\n",
        "text = model.generate_jit(random_key, params, 1000)[:, 0, 0].tolist()\n",
        "print(decode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hjAB9v31a78n",
        "outputId": "48a31082-53c4-4640-8f5c-eae8078a29bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 5.0033111572265625\t eval loss: 4.51181173324585\n",
            "Step: 100\t train loss: 3.08208966255188\t eval loss: 3.3525476455688477\n",
            "Step: 200\t train loss: 2.6820831298828125\t eval loss: 3.108391284942627\n",
            "Step: 300\t train loss: 2.530823230743408\t eval loss: 3.0190556049346924\n",
            "Step: 400\t train loss: 2.4697344303131104\t eval loss: 3.0656025409698486\n",
            "Step: 500\t train loss: 2.4076459407806396\t eval loss: 3.0052242279052734\n",
            "Step: 600\t train loss: 2.384352445602417\t eval loss: 3.0051612854003906\n",
            "Step: 700\t train loss: 2.3139853477478027\t eval loss: 2.9278626441955566\n",
            "Step: 800\t train loss: 2.2555510997772217\t eval loss: 2.8716318607330322\n",
            "Step: 900\t train loss: 2.179255485534668\t eval loss: 2.8671061992645264\n",
            "Step: 1000\t train loss: 2.092393636703491\t eval loss: 2.8011460304260254\n",
            "Step: 1100\t train loss: 2.050786018371582\t eval loss: 2.7552473545074463\n",
            "Step: 1200\t train loss: 1.9948629140853882\t eval loss: 2.7678041458129883\n",
            "Step: 1300\t train loss: 1.9232219457626343\t eval loss: 2.6877408027648926\n",
            "Step: 1400\t train loss: 1.8773834705352783\t eval loss: 2.637301445007324\n",
            "Step: 1500\t train loss: 1.820784330368042\t eval loss: 2.6574716567993164\n",
            "Step: 1600\t train loss: 1.7726376056671143\t eval loss: 2.610628128051758\n",
            "Step: 1700\t train loss: 1.7374048233032227\t eval loss: 2.6343142986297607\n",
            "Step: 1800\t train loss: 1.7171913385391235\t eval loss: 2.5992023944854736\n",
            "Step: 1900\t train loss: 1.6724435091018677\t eval loss: 2.518812417984009\n",
            "Step: 2000\t train loss: 1.6387214660644531\t eval loss: 2.543829917907715\n",
            "Step: 2100\t train loss: 1.6090223789215088\t eval loss: 2.4644336700439453\n",
            "Step: 2200\t train loss: 1.573716402053833\t eval loss: 2.47212553024292\n",
            "Step: 2300\t train loss: 1.5320415496826172\t eval loss: 2.472996234893799\n",
            "Step: 2400\t train loss: 1.522071361541748\t eval loss: 2.476703643798828\n",
            "Step: 2500\t train loss: 1.501800537109375\t eval loss: 2.43399715423584\n",
            "Step: 2600\t train loss: 1.5016448497772217\t eval loss: 2.4364774227142334\n",
            "Step: 2700\t train loss: 1.4422527551651\t eval loss: 2.4016056060791016\n",
            "Step: 2800\t train loss: 1.4399034976959229\t eval loss: 2.386258363723755\n",
            "Step: 2900\t train loss: 1.4109902381896973\t eval loss: 2.3700642585754395\n",
            "Step: 3000\t train loss: 1.3974311351776123\t eval loss: 2.3236217498779297\n",
            "Step: 3100\t train loss: 1.3975298404693604\t eval loss: 2.331057548522949\n",
            "Step: 3200\t train loss: 1.3717730045318604\t eval loss: 2.3510782718658447\n",
            "Step: 3300\t train loss: 1.3377457857131958\t eval loss: 2.3210840225219727\n",
            "Step: 3400\t train loss: 1.32928466796875\t eval loss: 2.356435775756836\n",
            "Step: 3500\t train loss: 1.319372534751892\t eval loss: 2.271847724914551\n",
            "Step: 3600\t train loss: 1.2967824935913086\t eval loss: 2.3161048889160156\n",
            "Step: 3700\t train loss: 1.307070016860962\t eval loss: 2.315549373626709\n",
            "Step: 3800\t train loss: 1.2949656248092651\t eval loss: 2.265345335006714\n",
            "Step: 3900\t train loss: 1.2649025917053223\t eval loss: 2.2733983993530273\n",
            "Step: 4000\t train loss: 1.2500755786895752\t eval loss: 2.3064208030700684\n",
            "Step: 4100\t train loss: 1.2420103549957275\t eval loss: 2.238286018371582\n",
            "Step: 4200\t train loss: 1.2295944690704346\t eval loss: 2.2544941902160645\n",
            "Step: 4300\t train loss: 1.2197365760803223\t eval loss: 2.2962899208068848\n",
            "Step: 4400\t train loss: 1.2287311553955078\t eval loss: 2.2228806018829346\n",
            "Step: 4500\t train loss: 1.187056541442871\t eval loss: 2.313401460647583\n",
            "Step: 4600\t train loss: 1.1804611682891846\t eval loss: 2.2717819213867188\n",
            "Step: 4700\t train loss: 1.1862468719482422\t eval loss: 2.282759189605713\n",
            "Step: 4800\t train loss: 1.1722246408462524\t eval loss: 2.2504186630249023\n",
            "Step: 4900\t train loss: 1.1734514236450195\t eval loss: 2.22857403755188\n",
            "Step: 5000\t train loss: 1.1637715101242065\t eval loss: 2.276742458343506\n",
            "Step: 5100\t train loss: 1.1409516334533691\t eval loss: 2.244457483291626\n",
            "Step: 5200\t train loss: 1.1297940015792847\t eval loss: 2.2379379272460938\n",
            "Step: 5300\t train loss: 1.1187405586242676\t eval loss: 2.210509777069092\n",
            "Step: 5400\t train loss: 1.1351711750030518\t eval loss: 2.2569620609283447\n",
            "Step: 5500\t train loss: 1.104339361190796\t eval loss: 2.2971925735473633\n",
            "Step: 5600\t train loss: 1.0961604118347168\t eval loss: 2.2484991550445557\n",
            "Step: 5700\t train loss: 1.0889447927474976\t eval loss: 2.310988426208496\n",
            "Step: 5800\t train loss: 1.0791609287261963\t eval loss: 2.3300302028656006\n",
            "Step: 5900\t train loss: 1.0709408521652222\t eval loss: 2.251009225845337\n",
            "Step: 6000\t train loss: 1.060619592666626\t eval loss: 2.332463264465332\n",
            "Step: 6100\t train loss: 1.0439568758010864\t eval loss: 2.241819381713867\n",
            "Step: 6200\t train loss: 1.0451343059539795\t eval loss: 2.2827229499816895\n",
            "Step: 6300\t train loss: 1.0305529832839966\t eval loss: 2.30173397064209\n",
            "Step: 6400\t train loss: 1.025863528251648\t eval loss: 2.3828349113464355\n",
            "Step: 6500\t train loss: 1.02392578125\t eval loss: 2.319761276245117\n",
            "Step: 6600\t train loss: 1.0123989582061768\t eval loss: 2.3875746726989746\n",
            "Step: 6700\t train loss: 1.0022945404052734\t eval loss: 2.3155927658081055\n",
            "Step: 6800\t train loss: 0.9995619654655457\t eval loss: 2.385420322418213\n",
            "Step: 6900\t train loss: 1.0046875476837158\t eval loss: 2.376985549926758\n",
            "Step: 7000\t train loss: 0.9881654977798462\t eval loss: 2.322192668914795\n",
            "Step: 7100\t train loss: 0.9764564037322998\t eval loss: 2.3814311027526855\n",
            "Step: 7200\t train loss: 0.9759804010391235\t eval loss: 2.409602642059326\n",
            "Step: 7300\t train loss: 0.9692283868789673\t eval loss: 2.447597026824951\n",
            "Step: 7400\t train loss: 0.9669023156166077\t eval loss: 2.3603591918945312\n",
            "Step: 7500\t train loss: 0.9399722814559937\t eval loss: 2.408795118331909\n",
            "Step: 7600\t train loss: 0.9483662843704224\t eval loss: 2.4499168395996094\n",
            "Step: 7700\t train loss: 0.9433032274246216\t eval loss: 2.428055763244629\n",
            "Step: 7800\t train loss: 0.9355940818786621\t eval loss: 2.4804930686950684\n",
            "Step: 7900\t train loss: 0.9225043654441833\t eval loss: 2.3841328620910645\n",
            "Step: 8000\t train loss: 0.912106990814209\t eval loss: 2.4860217571258545\n",
            "Step: 8100\t train loss: 0.9143197536468506\t eval loss: 2.451190948486328\n",
            "Step: 8200\t train loss: 0.9000159502029419\t eval loss: 2.4463376998901367\n",
            "Step: 8300\t train loss: 0.9022436141967773\t eval loss: 2.495570182800293\n",
            "Step: 8400\t train loss: 0.8858768939971924\t eval loss: 2.538405656814575\n",
            "Step: 8500\t train loss: 0.8836264610290527\t eval loss: 2.478304386138916\n",
            "Step: 8600\t train loss: 0.8742746114730835\t eval loss: 2.4874050617218018\n",
            "Step: 8700\t train loss: 0.8802509307861328\t eval loss: 2.444978713989258\n",
            "Step: 8800\t train loss: 0.8723196983337402\t eval loss: 2.5412509441375732\n",
            "Step: 8900\t train loss: 0.8551473617553711\t eval loss: 2.5486578941345215\n",
            "Step: 9000\t train loss: 0.853787899017334\t eval loss: 2.5436949729919434\n",
            "Step: 9100\t train loss: 0.8408457636833191\t eval loss: 2.56815505027771\n",
            "Step: 9200\t train loss: 0.8338653445243835\t eval loss: 2.4755802154541016\n",
            "Step: 9300\t train loss: 0.8263871669769287\t eval loss: 2.57527494430542\n",
            "Step: 9400\t train loss: 0.8245684504508972\t eval loss: 2.560255527496338\n",
            "Step: 9500\t train loss: 0.8121755719184875\t eval loss: 2.5894227027893066\n",
            "Step: 9600\t train loss: 0.821639358997345\t eval loss: 2.6186747550964355\n",
            "Step: 9700\t train loss: 0.8016524314880371\t eval loss: 2.6219515800476074\n",
            "Step: 9800\t train loss: 0.8037722110748291\t eval loss: 2.725040912628174\n",
            "Step: 9900\t train loss: 0.79319167137146\t eval loss: 2.6377639770507812\n",
            "Step: 10000\t train loss: 0.7895188927650452\t eval loss: 2.712400197982788\n",
            "Step: 10100\t train loss: 0.7825624942779541\t eval loss: 2.6883316040039062\n",
            "Step: 10200\t train loss: 0.7792381644248962\t eval loss: 2.702336072921753\n",
            "Step: 10300\t train loss: 0.7726771831512451\t eval loss: 2.703606367111206\n",
            "Step: 10400\t train loss: 0.7647080421447754\t eval loss: 2.661068916320801\n",
            "Step: 10500\t train loss: 0.7635680437088013\t eval loss: 2.7302584648132324\n",
            "Step: 10600\t train loss: 0.75577712059021\t eval loss: 2.6650190353393555\n",
            "Step: 10700\t train loss: 0.7429696917533875\t eval loss: 2.7723073959350586\n",
            "Step: 10800\t train loss: 0.7445782423019409\t eval loss: 2.7426319122314453\n",
            "Step: 10900\t train loss: 0.7448958158493042\t eval loss: 2.7262396812438965\n",
            "Step: 11000\t train loss: 0.7401436567306519\t eval loss: 2.76283860206604\n",
            "Step: 11100\t train loss: 0.7386690378189087\t eval loss: 2.8570351600646973\n",
            "Step: 11200\t train loss: 0.7246265411376953\t eval loss: 2.8439948558807373\n",
            "Step: 11300\t train loss: 0.7302154302597046\t eval loss: 2.7992653846740723\n",
            "Step: 11400\t train loss: 0.7290563583374023\t eval loss: 2.714400291442871\n",
            "Step: 11500\t train loss: 0.7115774154663086\t eval loss: 2.862776041030884\n",
            "Step: 11600\t train loss: 0.7048009634017944\t eval loss: 2.9083518981933594\n",
            "Step: 11700\t train loss: 0.7109071016311646\t eval loss: 2.915426015853882\n",
            "Step: 11800\t train loss: 0.6902158856391907\t eval loss: 2.8834848403930664\n",
            "Step: 11900\t train loss: 0.6952328085899353\t eval loss: 2.903838872909546\n",
            "Step: 12000\t train loss: 0.6910669803619385\t eval loss: 2.858336925506592\n",
            "Step: 12100\t train loss: 0.6791565418243408\t eval loss: 2.8418240547180176\n",
            "Step: 12200\t train loss: 0.6696639060974121\t eval loss: 2.834331750869751\n",
            "Step: 12300\t train loss: 0.67851722240448\t eval loss: 2.96008038520813\n",
            "Step: 12400\t train loss: 0.6692675948143005\t eval loss: 2.8976593017578125\n",
            "Step: 12500\t train loss: 0.6706274747848511\t eval loss: 2.9419169425964355\n",
            "Step: 12600\t train loss: 0.6764430403709412\t eval loss: 2.9065585136413574\n",
            "Step: 12700\t train loss: 0.6589195728302002\t eval loss: 2.9288954734802246\n",
            "Step: 12800\t train loss: 0.6617987751960754\t eval loss: 2.8984925746917725\n",
            "Step: 12900\t train loss: 0.646579921245575\t eval loss: 2.978342056274414\n",
            "Step: 13000\t train loss: 0.6461414694786072\t eval loss: 2.87380051612854\n",
            "Step: 13100\t train loss: 0.6468352675437927\t eval loss: 2.9932305812835693\n",
            "Step: 13200\t train loss: 0.6391632556915283\t eval loss: 3.0128140449523926\n",
            "Step: 13300\t train loss: 0.6246839761734009\t eval loss: 2.9771335124969482\n",
            "Step: 13400\t train loss: 0.6239256858825684\t eval loss: 3.0381574630737305\n",
            "Step: 13500\t train loss: 0.6182675361633301\t eval loss: 3.0289154052734375\n",
            "Step: 13600\t train loss: 0.609001636505127\t eval loss: 3.0560083389282227\n",
            "Step: 13700\t train loss: 0.6115456819534302\t eval loss: 3.024817705154419\n",
            "Step: 13800\t train loss: 0.6100841760635376\t eval loss: 3.010732650756836\n",
            "Step: 13900\t train loss: 0.5992735028266907\t eval loss: 3.1331167221069336\n",
            "Step: 14000\t train loss: 0.6002753376960754\t eval loss: 3.121790885925293\n",
            "Step: 14100\t train loss: 0.5957107543945312\t eval loss: 3.0491676330566406\n",
            "Step: 14200\t train loss: 0.588137149810791\t eval loss: 3.105152130126953\n",
            "Step: 14300\t train loss: 0.5855885744094849\t eval loss: 3.178561210632324\n",
            "Step: 14400\t train loss: 0.584907054901123\t eval loss: 3.1076698303222656\n",
            "Step: 14500\t train loss: 0.5879350900650024\t eval loss: 3.068188190460205\n",
            "Step: 14600\t train loss: 0.5819097757339478\t eval loss: 3.1615149974823\n",
            "Step: 14700\t train loss: 0.5749236345291138\t eval loss: 3.163029670715332\n",
            "Step: 14800\t train loss: 0.5632092952728271\t eval loss: 3.2201249599456787\n",
            "Step: 14900\t train loss: 0.5784091949462891\t eval loss: 3.1739776134490967\n",
            "Step: 15000\t train loss: 0.5609498023986816\t eval loss: 3.237809181213379\n",
            "Step: 15100\t train loss: 0.5627621412277222\t eval loss: 3.2124786376953125\n",
            "Step: 15200\t train loss: 0.5543537735939026\t eval loss: 3.1596169471740723\n",
            "Step: 15300\t train loss: 0.5607194900512695\t eval loss: 3.2825069427490234\n",
            "Step: 15400\t train loss: 0.5539966225624084\t eval loss: 3.109590530395508\n",
            "Step: 15500\t train loss: 0.5597262382507324\t eval loss: 3.3182785511016846\n",
            "Step: 15600\t train loss: 0.5471093654632568\t eval loss: 3.2892940044403076\n",
            "Step: 15700\t train loss: 0.5317466259002686\t eval loss: 3.2338664531707764\n",
            "Step: 15800\t train loss: 0.5390946269035339\t eval loss: 3.2555103302001953\n",
            "Step: 15900\t train loss: 0.5314369201660156\t eval loss: 3.114410877227783\n",
            "Step: 16000\t train loss: 0.5331965684890747\t eval loss: 3.186979293823242\n",
            "Step: 16100\t train loss: 0.5326414108276367\t eval loss: 3.2413082122802734\n",
            "Step: 16200\t train loss: 0.5297125577926636\t eval loss: 3.2678349018096924\n",
            "Step: 16300\t train loss: 0.527379035949707\t eval loss: 3.32647967338562\n",
            "Step: 16400\t train loss: 0.5286931991577148\t eval loss: 3.261777877807617\n",
            "Step: 16500\t train loss: 0.5140263438224792\t eval loss: 3.356398820877075\n",
            "Step: 16600\t train loss: 0.5218119025230408\t eval loss: 3.398974895477295\n",
            "Step: 16700\t train loss: 0.5071253776550293\t eval loss: 3.423330545425415\n",
            "Step: 16800\t train loss: 0.5101045370101929\t eval loss: 3.306046485900879\n",
            "Step: 16900\t train loss: 0.5049742460250854\t eval loss: 3.297858238220215\n",
            "Step: 17000\t train loss: 0.4951153099536896\t eval loss: 3.3515164852142334\n",
            "Step: 17100\t train loss: 0.5075880289077759\t eval loss: 3.4104418754577637\n",
            "Step: 17200\t train loss: 0.503258466720581\t eval loss: 3.300194263458252\n",
            "Step: 17300\t train loss: 0.4969669580459595\t eval loss: 3.3364410400390625\n",
            "Step: 17400\t train loss: 0.49277448654174805\t eval loss: 3.29848575592041\n",
            "Step: 17500\t train loss: 0.49275779724121094\t eval loss: 3.394890069961548\n",
            "Step: 17600\t train loss: 0.4885849356651306\t eval loss: 3.344615936279297\n",
            "Step: 17700\t train loss: 0.48803335428237915\t eval loss: 3.434277057647705\n",
            "Step: 17800\t train loss: 0.48044419288635254\t eval loss: 3.502732753753662\n",
            "Step: 17900\t train loss: 0.47601431608200073\t eval loss: 3.3872499465942383\n",
            "Step: 18000\t train loss: 0.48175373673439026\t eval loss: 3.406920909881592\n",
            "Step: 18100\t train loss: 0.4760304391384125\t eval loss: 3.4412641525268555\n",
            "Step: 18200\t train loss: 0.4785810112953186\t eval loss: 3.578516960144043\n",
            "Step: 18300\t train loss: 0.47318583726882935\t eval loss: 3.380786180496216\n",
            "Step: 18400\t train loss: 0.4634944200515747\t eval loss: 3.5097084045410156\n",
            "Step: 18500\t train loss: 0.4725680351257324\t eval loss: 3.4538605213165283\n",
            "Step: 18600\t train loss: 0.4670710563659668\t eval loss: 3.4197347164154053\n",
            "Step: 18700\t train loss: 0.46058523654937744\t eval loss: 3.5391130447387695\n",
            "Step: 18800\t train loss: 0.45970404148101807\t eval loss: 3.4521169662475586\n",
            "Step: 18900\t train loss: 0.4585638642311096\t eval loss: 3.5065722465515137\n",
            "Step: 19000\t train loss: 0.4534342885017395\t eval loss: 3.556551218032837\n",
            "Step: 19100\t train loss: 0.4530183672904968\t eval loss: 3.6120290756225586\n",
            "Step: 19200\t train loss: 0.4428834319114685\t eval loss: 3.4417648315429688\n",
            "Step: 19300\t train loss: 0.45272937417030334\t eval loss: 3.557873487472534\n",
            "Step: 19400\t train loss: 0.44136375188827515\t eval loss: 3.513059139251709\n",
            "Step: 19500\t train loss: 0.44640886783599854\t eval loss: 3.5013434886932373\n",
            "Step: 19600\t train loss: 0.4378763735294342\t eval loss: 3.5256364345550537\n",
            "Step: 19700\t train loss: 0.440836638212204\t eval loss: 3.579723834991455\n",
            "Step: 19800\t train loss: 0.44704514741897583\t eval loss: 3.433138847351074\n",
            "Step: 19900\t train loss: 0.43918877840042114\t eval loss: 3.5531816482543945\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b8192a26a9ae>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mparams_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./outputs/params.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparams_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/params.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_state_dict = serialization.to_state_dict(state.params)\n",
        "with open(\"/content/outputs/params.pickle\", \"wb\") as params_file:\n",
        "    pickle.dump(params_state_dict, params_file)\n",
        "\n",
        "# Let's now generate some text\n",
        "model = Model(num_layers=config.num_layers, num_heads=config.num_heads, head_size=config.head_size)\n",
        "params = model.init(\n",
        "    random_key, jnp.ones((config.batch_size, config.block_size), dtype=jnp.int32), training=False\n",
        ")\n",
        "with open(\"/content/outputs/params.pickle\", \"rb\") as params_file:\n",
        "    params_state_dict = pickle.load(params_file)\n",
        "params = serialization.from_state_dict(params, params_state_dict)\n",
        "\n",
        "text = model.generate_jit(random_key, params, 1000)[:, 0, 0].tolist()\n",
        "print(decode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpvUCwyxlGw",
        "outputId": "c3e857eb-f322-4ed6-da05-2d8942ac099b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER IV.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "Alice was silent.\n",
            "\n",
            "The King, and the Queen jumped up, but into the air.\n",
            "\n",
            "ââas far out to sea as you canââ\n",
            "\n",
            "âSwim after them!â screamed the Gryphon, and, taking Alice by the hand, it had no very clear notion how long ago\n",
            "anything happened.) So she began again: âOÃ¹ est ma chatte?â which\n",
            "was the first sentence in her French lesson-book. The Mouse gave a\n",
            "sudden leap out of the water, and seemed to quiver all over with\n",
            "fright. âOh, I beg your pardon!â cried Alice hastily, afraid that she\n",
            "had hurt the poor little thing was sobbing of the Nile\n",
            "    On every golden scale!\n",
            "\n",
            "âHow cheerfully he seems to go leave the court_.â\n",
            "\n",
            "Everybody looked at Alice.\n",
            "\n",
            "â_Iâm_ not a mile high,â said Alice, looking down with wonder\n",
            "at the Mouse was speaking, so that her idea of the\n",
            "tale was something like this:â\n",
            "\n",
            "            âI feared it might injure the brain;\n",
            "But, now that Iâm perfectly sure I have none,\n",
            "    Why, I do it again and again, and a\n"
          ]
        }
      ]
    }
  ]
}